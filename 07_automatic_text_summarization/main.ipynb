{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import pprint\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "8 Open Source Big Data Tools to use in 2018\n",
    "Go to the profile of Vladimir Fedak\n",
    "Vladimir Fedak\n",
    "Aug 29, 2018\n",
    "Big Data analytics is an essential part of any business workflow nowadays. To make the most of it, we recommend using these popular open source Big Data solutions for each stage of data processing.\n",
    "\n",
    "Why opting for open source Big Data tools and not for proprietary solutions, you might ask? The reason became obvious over the last decade — open sourcing the software is the way to make it popular.\n",
    "\n",
    "Developers prefer to avoid vendor lock-in and tend to use free tools for the sake of versatility, as well as due to the possibility to contribute to the evolvement of their beloved platform. Open source products boast the same, if not better level of documentation depth, along with a much more dedicated support from the community, who are also the product developers and Big Data practitioners, who know what they need from a product. Thus said, this is the list of 8 hot Big Data tool to use in 2018, based on popularity, feature richness and usefulness.\n",
    "\n",
    "1. Apache Hadoop\n",
    "The long-standing champion in the field of Big Data processing, well-known for its capabilities for huge-scale data processing. This open source Big Data framework can run on-prem or in the cloud and has quite low hardware requirements. The main Hadoop benefits and features are as follows:\n",
    "\n",
    "HDFS — Hadoop Distributed File System, oriented at working with huge-scale bandwidth\n",
    "MapReduce — a highly configurable model for Big Data processing\n",
    "YARN — a resource scheduler for Hadoop resource management\n",
    "Hadoop Libraries — the needed glue for enabling third party modules to work with Hadoop\n",
    "2. Apache Spark\n",
    "Apache Spark is the alternative — and in many aspects the successor — of Apache Hadoop. Spark was built to address the shortcomings of Hadoop and it does this incredibly well. For example, it can process both batch data and real-time data, and operates 100 times faster than MapReduce. Spark provides the in-memory data processing capabilities, which is way faster than disk processing leveraged by MapReduce. In addition, Spark works with HDFS, OpenStack and Apache Cassandra, both in the cloud and on-prem, adding another layer of versatility to big data operations for your business.\n",
    "\n",
    "3. Apache Storm\n",
    "Storm is another Apache product, a real-time framework for data stream processing, which supports any programming language. Storm scheduler balances the workload between multiple nodes based on topology configuration and works well with Hadoop HDFS. Apache Storm has the following benefits:\n",
    "\n",
    "Great horizontal scalability\n",
    "Built-in fault-tolerance\n",
    "Auto-restart on crashes\n",
    "Clojure-written\n",
    "Works with Direct Acyclic Graph(DAG) topology\n",
    "Output files are in JSON format\n",
    "4. Apache Cassandra\n",
    "Apache Cassandra is one of the pillars behind Facebook’s massive success, as it allows to process structured data sets distributed across huge number of nodes across the globe. It works well under heavy workloads due to its architecture without single points of failure and boasts unique capabilities no other NoSQL or relational DB has, such as:\n",
    "\n",
    "Great liner scalability\n",
    "Simplicity of operations due to a simple query language used\n",
    "Constant replication across nodes\n",
    "Simple adding and removal of nodes from a running cluster\n",
    "High fault tolerance\n",
    "Built-in high-availability\n",
    "5. MongoDB\n",
    "MongoDB is another great example of an open source NoSQL database with rich features, which is cross-platform compatible with many programming languages. IT Svit uses MongoDB in a variety of cloud computing and monitoring solutions, and we specifically developed a module for automated MongoDB backups using Terraform. The most prominent MongoDB features are:\n",
    "\n",
    "Stores any type of data, from text and integer to strings, arrays, dates and boolean\n",
    "Cloud-native deployment and great flexibility of configuration\n",
    "Data partitioning across multiple nodes and data centers\n",
    "Significant cost savings, as dynamic schemas enable data processing on the go\n",
    "6. R Programming Environment\n",
    "R is mostly used along with JuPyteR stack (Julia, Python, R) for enabling wide-scale statistical analysis and data visualization. JupyteR Notebook is one of 4 most popular Big Data visualization tools, as it allows composing literally any analytical model from more than 9,000 CRAN (Comprehensive R Archive Network) algorithms and modules, running it in a convenient environment, adjusting it on the go and inspecting the analysis results at once. The main benefits of using R are as follows:\n",
    "\n",
    "R can run inside the SQL server\n",
    "R runs on both Windows and Linux servers\n",
    "R supports Apache Hadoop and Spark\n",
    "R is highly portable\n",
    "R easily scales from a single test machine to vast Hadoop data lakes\n",
    "7. Neo4j\n",
    "Neo4j is an open source graph database with interconnected node-relationship of data, which follows the key-value pattern in storing data. IT Svit has recently built a resilient AWS infrastructure with Neo4j for one of our customers and the database performs well under heavy workload of network data and graph-related requests. Main Neo4j features are as follows:\n",
    "\n",
    "Built-in support for ACID transactions\n",
    "Cypher graph query language\n",
    "High-availability and scalability\n",
    "Flexibility due to the absence of schemas\n",
    "Integration with other databases\n",
    "8. Apache SAMOA\n",
    "This is another of the Apache family of tools used for Big Data processing. Samoa specializes at building distributed streaming algorithms for successful Big Data mining. This tool is built with pluggable architecture and must be used atop other Apache products like Apache Storm we mentioned earlier. Its other features used for Machine Learning include the following:\n",
    "\n",
    "Clustering\n",
    "Classification\n",
    "Normalization\n",
    "Regression\n",
    "Programming primitives for building custom algorithms\n",
    "Using Apache Samoa enables the distributed stream processing engines to provide such tangible benefits:\n",
    "\n",
    "Program once, use anywhere\n",
    "Reuse the existing infrastructure for new projects\n",
    "No reboot or deployment downtime\n",
    "No need for backups or time-consuming updates\n",
    "Final thoughts on the list of hot Big Data tools for 2018\n",
    "Big Data industry and data science evolve rapidly and progressed a big deal lately, with multiple Big Data projects and tools launched in 2017. This is one of the hottest IT trends of 2018, along with IoT, blockchain, AI & ML.\n",
    "\n",
    "Big Data analytics is increasingly widespread in multiple industries, from using ML in banking and financial services to healthcare and government, and open source Big Data tools are the mainframe of any Big Data architect’s toolkit. In case you have any difficulties with Big Data implementation — don’t hesitate to contact IT Svit, we would be glad to help!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sentences = []\n",
    "candidate_sentences = {}\n",
    "candidate_sentence_count = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse text.\n",
    "striptext = text.replace('\\r', '').replace('\\n', '')\n",
    "words = word_tokenize(striptext)\n",
    "words_lower = [word.lower() \n",
    "               for word in words \n",
    "               if word not in stopwords.words() \n",
    "               and word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 20 most frequent words.\n",
    "word_frequencies = FreqDist(words_lower)\n",
    "most_frequent_words = FreqDist(words_lower).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('data', 36),\n",
      "    ('big', 18),\n",
      "    ('apache', 14),\n",
      "    ('open', 9),\n",
      "    ('source', 8),\n",
      "    ('tools', 8),\n",
      "    ('processing', 8),\n",
      "    ('hadoop', 8),\n",
      "    ('well', 5),\n",
      "    ('features', 5),\n",
      "    ('it', 5),\n",
      "    ('r', 5),\n",
      "    ('use', 4),\n",
      "    ('using', 4),\n",
      "    ('the', 4),\n",
      "    ('due', 4),\n",
      "    ('benefits', 4),\n",
      "    ('follows', 4),\n",
      "    ('distributed', 4),\n",
      "    ('spark', 4)]\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(most_frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sentence, create a dictionary.\n",
    "sentences = sent_tokenize(striptext)\n",
    "for sent in sentences:\n",
    "    candidate_sentences[sent] = sent.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the sentence with the most important words.\n",
    "for long, short in candidate_sentences.items():\n",
    "    count = 0\n",
    "    for freq_word, freq_score in most_frequent_words:\n",
    "        if freq_word in short:\n",
    "            count += freq_score\n",
    "            candidate_sentence_count[long] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([   (   'Its other features used for Machine Learning include the '\n",
      "                    'following:ClusteringClassificationNormalizationRegressionProgramming '\n",
      "                    'primitives for building custom algorithmsUsing Apache '\n",
      "                    'Samoa enables the distributed stream processing engines '\n",
      "                    'to provide such tangible benefits:Program once, use '\n",
      "                    'anywhereReuse the existing infrastructure for new '\n",
      "                    'projectsNo reboot or deployment downtimeNo need for '\n",
      "                    'backups or time-consuming updatesFinal thoughts on the '\n",
      "                    'list of hot Big Data tools for 2018Big Data industry and '\n",
      "                    'data science evolve rapidly and progressed a big deal '\n",
      "                    'lately, with multiple Big Data projects and tools '\n",
      "                    'launched in 2017.',\n",
      "                    119),\n",
      "                (   'The main Hadoop benefits and features are as '\n",
      "                    'follows:HDFS\\u200a—\\u200aHadoop Distributed File System, '\n",
      "                    'oriented at working with huge-scale '\n",
      "                    'bandwidthMapReduce\\u200a—\\u200aa highly configurable '\n",
      "                    'model for Big Data processingYARN\\u200a—\\u200aa resource '\n",
      "                    'scheduler for Hadoop resource managementHadoop '\n",
      "                    'Libraries\\u200a—\\u200athe needed glue for enabling third '\n",
      "                    'party modules to work with Hadoop2.',\n",
      "                    109),\n",
      "                (   'To make the most of it, we recommend using these popular '\n",
      "                    'open source Big Data solutions for each stage of data '\n",
      "                    'processing.Why opting for open source Big Data tools and '\n",
      "                    'not for proprietary solutions, you might ask?',\n",
      "                    105),\n",
      "                (   'Apache HadoopThe long-standing champion in the field of '\n",
      "                    'Big Data processing, well-known for its capabilities for '\n",
      "                    'huge-scale data processing.',\n",
      "                    103)])\n"
     ]
    }
   ],
   "source": [
    "sorted_sentences = OrderedDict(sorted(candidate_sentence_count.items(), \n",
    "                                      key=lambda t: t[1],\n",
    "                                      reverse=True)[:4])\n",
    "pp.pprint(sorted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main Hadoop benefits and features are as follows:HDFS\\u200a—\\u200aHadoop Distributed File System, oriented at working with huge-scale bandwidthMapReduce\\u200a—\\u200aa highly configurable model for Big Data processingYARN\\u200a—\\u200aa resource scheduler for Hadoop resource managementHadoop Libraries\\u200a—\\u200athe needed glue for enabling third party modules to work with Hadoop2.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summarize(striptext, word_count=50)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "apache\n",
      "big\n",
      "processing\n",
      "process\n",
      "hadoop benefits\n",
      "graph\n",
      "spark\n",
      "scales\n",
      "times\n",
      "time\n",
      "great\n",
      "mongodb\n",
      "run\n",
      "running\n",
      "runs\n",
      "use\n",
      "usefulness\n",
      "uses\n",
      "node\n",
      "platform\n",
      "products\n",
      "source\n",
      "sourcing\n",
      "distributed\n",
      "product developers\n",
      "query\n",
      "developed\n",
      "follows\n",
      "following\n",
      "open\n",
      "fault\n",
      "multiple nodes\n",
      "language\n",
      "languages\n",
      "tools\n",
      "tool\n",
      "popular\n",
      "enabling\n",
      "enable\n",
      "enables\n",
      "database\n",
      "databases\n",
      "samoa\n",
      "composing\n",
      "hardware\n",
      "arrays\n",
      "projectsno\n",
      "environment\n",
      "adding\n",
      "vendor\n",
      "hdfs\n",
      "main\n",
      "configurable\n",
      "configuration\n",
      "popularity feature\n",
      "stream\n",
      "streaming\n",
      "jupyter\n",
      "solutions\n",
      "support\n",
      "supports\n",
      "aws infrastructure\n",
      "features\n",
      "scheduler\n",
      "programming\n"
     ]
    }
   ],
   "source": [
    "print(keywords(striptext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = 'english'\n",
    "SENTENCES_COUNT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "..\n",
      "sampleText.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "ls -a data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PlaintextParser.from_file('data/sampleText.txt', \n",
    "                                   Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luhn Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luhn's 1958 paper \"The automatic creation of literature abstracts,\" describes a text summarization method that will \"save a prospective reader time and effort in finding useful information in a given article or report\" and that the problem of finding information \"is being aggravated by the ever-increasing output of technical literature.\"\n",
      "\n",
      "With this early work, Luhn proposed a text summarization method where the computer would read each sentence in a paper, extract the frequently-occurring words, which he calls significant words, and then look for the sentences that had the most examples of those significant words.\n",
      "\n",
      "As long as the amount of text that is extracted is a subset of the original text, this type of summarization achieves the goal of compressing the original text into a shorter size.\n",
      "\n",
      "In this chapter we will focus on summarization techniques for text documents, but researchers are also working on summarization algorithms designed for video, images, sound, and more.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "luhn = LuhnSummarizer(stemmer)\n",
    "luhn.stop_wrods = get_stop_words(LANGUAGE)\n",
    "for sent in luhn(parser.document, SENTENCES_COUNT):\n",
    "    print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this early work, Luhn proposed a text summarization method where the computer would read each sentence in a paper, extract the frequently-occurring words, which he calls significant words, and then look for the sentences that had the most examples of those significant words.\n",
      "\n",
      "In an extractive summarization method, the summary is comprised of words, phrases, or sentences that are drawn directly from the original text.\n",
      "\n",
      "As long as the amount of text that is extracted is a subset of the original text, this type of summarization achieves the goal of compressing the original text into a shorter size.\n",
      "\n",
      "In this chapter we will focus on summarization techniques for text documents, but researchers are also working on summarization algorithms designed for video, images, sound, and more.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textrank = TextRankSummarizer(stemmer)\n",
    "textrank.stop_words = get_stop_words(LANGUAGE)\n",
    "for sent in textrank(parser.document, SENTENCES_COUNT):\n",
    "    print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the academic literature, text summarization is often proposed as a solution to information overload, and we in the 21st century like to think that we are uniquely positioned in history in having to deal with this problem.\n",
      "\n",
      "In an extractive summarization method, the summary is comprised of words, phrases, or sentences that are drawn directly from the original text.\n",
      "\n",
      "Alternatively, an abstractive summarization attempts to distill the key ideas in a text and repackage them into a human-readable, and usually shorter, synthesis.\n",
      "\n",
      "However, since the goal is to create a summary, abstractive methods must also reduce the length of the text while focusing on only retaining the most important concepts in it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lsa = LsaSummarizer(stemmer)\n",
    "lsa.stop_words = get_stop_words(LANGUAGE)\n",
    "for sent in lsa(parser.document, SENTENCES_COUNT):\n",
    "    print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edmundson Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the academic literature, text summarization is often proposed as a solution to information overload, and we in the 21st century like to think that we are uniquely positioned in history in having to deal with this problem.\n",
      "\n",
      "However, since the goal is to create a summary, abstractive methods must also reduce the length of the text while focusing on only retaining the most important concepts in it.\n",
      "\n",
      "In this chapter we will focus on summarization techniques for text documents, but researchers are also working on summarization algorithms designed for video, images, sound, and more.\n",
      "\n",
      "In the next section we will review some of the currently available text summarization libraries and applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ed = EdmundsonSummarizer(stemmer)\n",
    "ed.bonus_words = ('focus', 'proposed', 'method', 'describes') # Points to important sentences.\n",
    "ed.stigma_words = ('example') # The opposite of bonus words.\n",
    "ed.null_words = ('literature', 'however') # Neutral/irrelevant words.\n",
    "for sent in ed(parser.document, SENTENCES_COUNT):\n",
    "    print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
